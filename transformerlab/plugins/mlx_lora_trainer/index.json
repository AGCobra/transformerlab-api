{
    "name": "Apple MLX LoRA Trainer",
    "uniqueId": "mlx_lora_trainer",
    "description": "MLX Machine learning research on your laptop or in a data center - by Apple",
    "plugin-format": "python",
    "type": "trainer",
    "version": "0.1.1",
    "model_architectures": [
        "LlamaForCausalLM",
        "MistralForCausalLM",
        "PhiForCausalLM",
        "MLX"
    ],
    "files": [
        "main.py",
        "setup.sh"
    ],
    "setup-script": "setup.sh",
    "parameters": {
        "lora_layers": {
            "title": "LoRA Layers",
            "type": "integer",
            "default": 16,
            "minimum": 4,
            "maximum": 64
        },
        "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "default": 5e-5,
            "minimum": 1e-6,
            "maximum": 1e+6
        },
        "iters": {
            "title": "Iterations",
            "type": "integer",
            "default": 1000,
            "minimum": 1,
            "maximum": 1000000
        },
        "steps_per_report": {
            "title": "Steps per Report",
            "type": "integer",
            "default": 100,
            "minimum": 1
        },
        "steps_per_eval": {
            "title": "Steps per Evaluation",
            "type": "integer",
            "default": 100,
            "minimum": 1
        },
        "save_every": {
            "title": "Save Every",
            "type": "integer",
            "default": 100,
            "minimum": 1
        },
        "adaptor_name": {
            "title": "Adaptor Name",
            "type": "string",
            "required": true
        }
    },
    "parameters_ui": {
        "lora_layers": {
            "ui:help": "Number of layers to fine-tune. The default is 16, so you can try 8 or 4. This reduces the amount of memory needed for back propagation. It may also reduce the quality of the fine-tuned model if you are fine-tuning with a lot of data."
        },
        "learning_rate": {
            "ui:help": "Adam Learning rate."
        },
        "iters": {
            "ui:help": "Number of iterations (not epochs) to train -- 1000 could be a starting point."
        },
        "steps_per_report": {
            "ui:help": "Number of training steps between loss reporting."
        }
    }
}