{
    "name": "Llama-cpp Server using FastChat",
    "uniqueId": "llama_cpp_server_fastchat",
    "description": "Runs llama-cpp-python using FastChat as the worker engine",
    "plugin-format": "python",
    "type": "loader",
    "model_architectures": [
        "GGUF"
    ],
    "files": [
        "main.py",
        "setup.sh"
    ],
    "setup-script": "setup.sh",
    "parameters": []
}