{
    "name": "vLLM Server",
    "uniqueId": "vllm_server",
    "description": "vLLM is a fast and easy-to-use library for LLM inference and serving.",
    "plugin-format": "python",
    "type": "loader",
    "model_architectures": [
        "LlamaForCausalLM",
        "T5ForConditionalGeneration",
        "FalconForCausalLM",
        "MistralForCausalLM",
        "GPTBigCodeForCausalLM"
    ],
    "files": [
        "main.py",
        "setup.sh"
    ],
    "setup-script": "setup.sh",
    "parameters": []
}